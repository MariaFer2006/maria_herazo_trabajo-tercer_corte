---
layout: default
title: "An√°lisis de Flujo de Datos Simulado con Spark"
date: 2025-10-29
author: Maria Fernanda Herazo Escobar
categories: [analytics, spark, streaming]
---

# üîç An√°lisis de Flujo de Datos Simulado con Spark

## Escenario Empresarial

Imagina una **tienda online** que necesita analizar en tiempo real el comportamiento de navegaci√≥n de sus usuarios. Cada clic, cada sesi√≥n, cada interacci√≥n genera datos valiosos que pueden revelar patrones de compra, identificar usuarios problem√°ticos o detectar oportunidades de negocio.

Este proyecto simula ese escenario usando **Apache Spark** para procesar un flujo continuo de eventos de clickstream.

---

## üìä Dataset Utilizado

El dataset `clickstream_data.csv` contiene 1000 registros simulados con la siguiente estructura:

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `Timestamp` | datetime | Momento exacto del evento |
| `User_ID` | string | Identificador √∫nico del usuario (User_001 a User_050) |
| `Clicks` | integer | N√∫mero de clics en esa ventana temporal (1-5) |

**Ejemplo de datos:**
```
Timestamp,User_ID,Clicks
2025-10-29 19:01:04,User_034,3
2025-10-29 19:01:07,User_018,3
2025-10-29 19:01:12,User_030,2
```

---

## ‚öôÔ∏è Configuraci√≥n de Spark

### C√≥digo de Inicializaci√≥n

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_timestamp, sum as spark_sum, col, window

# Inicializar sesi√≥n de Spark
spark = SparkSession.builder \
    .appName("ClickstreamAnalysis") \
    .getOrCreate()

# Cargar datos
df = spark.read.csv("assets/data/clickstream_data.csv", 
                    header=True, 
                    inferSchema=True)

# Convertir Timestamp a formato datetime
df = df.withColumn("Timestamp", to_timestamp(col("Timestamp")))
```

### Procesamiento por Ventanas de Tiempo

Simulamos streaming agrupando eventos en **ventanas de 1 minuto**:

```python
# Agregar clicks por ventanas de 1 minuto
windowed = df.groupBy(
    window("Timestamp", "1 minute"), 
    "User_ID"
).agg(
    spark_sum("Clicks").alias("clicks_ventana")
)

# Total de clicks por usuario
total_por_usuario = df.groupBy("User_ID") \
    .agg(spark_sum("Clicks").alias("total_clicks")) \
    .orderBy(col("total_clicks").desc())
```

---

## üìà Visualizaciones y An√°lisis

### 1. Top 15 Usuarios por Actividad

![Top 15 Usuarios]({{ "/assets/images/top_users_chart.png" | relative_url }})

**Insight:** Los usuarios `User_001`, `User_006` y `User_026` concentran la mayor actividad. Representan oportunidades de fidelizaci√≥n premium.

### 2. An√°lisis Temporal de Clicks

![An√°lisis Temporal]({{ "/assets/images/temporal_analysis.png" | relative_url }})

**Patr√≥n detectado:** Se observan **picos de actividad cada 5-10 minutos**, lo que sugiere sesiones de navegaci√≥n intermitente. Ideal para:
- Auto-escalado de infraestructura durante picos
- Activaci√≥n de ofertas flash en ventanas de alta demanda

### 3. Relaci√≥n Clicks vs Sesiones

![Clicks vs Sesiones]({{ "/assets/images/clicks_vs_sessions.png" | relative_url }})

**Hallazgo:** Existe una correlaci√≥n positiva entre n√∫mero de sesiones y total de clicks. Usuarios con m√°s de 30 sesiones tienden a convertir mejor.

### 4. Distribuci√≥n de Usuarios

![Distribuci√≥n de Usuarios]({{ "/assets/images/user_distribution.png" | relative_url }})

**Segmentaci√≥n identificada:**
- **Exploradores** (1-10 sesiones): 60% de usuarios, bajo engagement
- **Regulares** (11-25 sesiones): 30% de usuarios, engagement moderado
- **Power Users** (26+ sesiones): 10% de usuarios, ¬°generan el 45% del tr√°fico!

### 5. Mapa de Calor de Actividad

![Mapa de Calor]({{ "/assets/images/activity_heatmap.png" | relative_url }})

**Conclusi√≥n:** La actividad se concentra entre las **19:00-20:00 hrs**, momento √≥ptimo para campa√±as de marketing en tiempo real.

---

## üéØ ¬øQu√© Patrones Encontramos?

### 1. **Ley de Pareto en Acci√≥n** (Regla 80/20)
El 20% de los usuarios (power users) generan aproximadamente el **45% del tr√°fico total**. Estos usuarios son:
- Candidatos ideales para programas de lealtad
- Susceptibles a ofertas personalizadas
- Potenciales embajadores de marca

### 2. **Sesiones Bimodales**
Detectamos dos tipos de comportamiento claramente diferenciados:
- **Sesiones exploratorias**: 1-3 clicks, alta tasa de rebote
- **Sesiones comprometidas**: 5+ clicks, mayor intenci√≥n de compra

### 3. **Patrones Temporales Predecibles**
Los picos de actividad cada 5-10 minutos permiten:
- Predicci√≥n de carga para auto-escalado
- Pre-carga de cache inteligente
- Activaci√≥n de promociones din√°micas

---

## üíº ¬øC√≥mo Ayuda Esto a la Tienda?

### **Decisiones Basadas en Datos en Tiempo Real**

| Problema de Negocio | Soluci√≥n con Streaming Analytics |
|---------------------|----------------------------------|
| üéØ **Retenci√≥n** | Detectar usuarios con se√±ales de abandono (baja actividad s√∫bita) y activar ofertas autom√°ticas |
| üì¶ **Inventario** | Predecir demanda basada en patrones de clicks en categor√≠as |
| üí∞ **Pricing din√°mico** | Ajustar precios seg√∫n demanda en tiempo real |
| üîç **Personalizaci√≥n** | Recomendar productos basados en comportamiento de usuarios similares |
| ‚ö° **Infraestructura** | Auto-escalar recursos durante picos detectados con 5 min de anticipaci√≥n |

### **ROI Estimado:**
- üìà +15% en conversi√≥n por personalizaci√≥n en tiempo real
- üíµ -30% en costos de infraestructura por escalado predictivo
- üéÅ +25% en engagement por ofertas oportunas

---

## üèóÔ∏è Arquitectura del Blog

### Estructura del Proyecto

```
blog-analytics/
‚îú‚îÄ‚îÄ _config.yml              # Configuraci√≥n Jekyll + Tema Cayman
‚îú‚îÄ‚îÄ _includes/               # Componentes reutilizables
‚îÇ   ‚îú‚îÄ‚îÄ head.html           # Meta tags, CSS
‚îÇ   ‚îî‚îÄ‚îÄ footer.html         # Pie de p√°gina
‚îú‚îÄ‚îÄ _layouts/                # Plantillas
‚îÇ   ‚îú‚îÄ‚îÄ default.html        # Layout principal
‚îÇ   ‚îî‚îÄ‚îÄ post.html           # Layout de art√≠culos
‚îú‚îÄ‚îÄ _posts/                  # Contenido del blog
‚îÇ   ‚îî‚îÄ‚îÄ 2025-10-29-analisis-clickstream-spark.md
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css       # Estilos personalizados
‚îÇ   ‚îú‚îÄ‚îÄ images/             # Gr√°ficas generadas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ top_users_chart.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ temporal_analysis.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clicks_vs_sessions.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_distribution.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ activity_heatmap.png
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îî‚îÄ‚îÄ clickstream_data.csv
‚îú‚îÄ‚îÄ generate_graphs.py       # Script Python para gr√°ficas
‚îú‚îÄ‚îÄ Gemfile                  # Dependencias Ruby
‚îî‚îÄ‚îÄ index.md                 # P√°gina principal
```

### Tecnolog√≠as Utilizadas

- **Jekyll 4.3.0**: Generador de sitios est√°ticos
- **Tema Cayman**: Dise√±o limpio y profesional
- **Apache Spark (PySpark)**: Procesamiento distribuido
- **Matplotlib + Pandas**: Visualizaci√≥n de datos
- **GitHub Pages**: Hosting gratuito

---

## üöÄ Proceso de Despliegue

### **Paso 1: Instalaci√≥n Local**

```bash
# Instalar Ruby y Bundler
gem install bundler jekyll

# Instalar dependencias del proyecto
bundle install

# Instalar librer√≠as Python
pip install matplotlib pandas numpy seaborn
```

### **Paso 2: Generar Visualizaciones**

```bash
# Ejecutar script de gr√°ficas
python generate_graphs.py

# Verificar que se crearon las im√°genes
ls assets/images/
# Output: top_users_chart.png, temporal_analysis.png, etc.
```

### **Paso 3: Servidor Local**

```bash
# Iniciar Jekyll
bundle exec jekyll serve --livereload

# Acceder en navegador
# http://localhost:4000
```

### **Paso 4: Despliegue en GitHub Pages**

```bash
# Opci√≥n A: Repositorio personal (username.github.io)
git init
git add .
git commit -m "Blog de anal√≠tica avanzada"
git branch -M main
git remote add origin https://github.com/username/username.github.io.git
git push -u origin main

# El sitio estar√° disponible en:
# https://username.github.io
```

```bash
# Opci√≥n B: Repositorio de proyecto
# 1. Crear repo en GitHub (ej: "blog-analytics")
# 2. Actualizar _config.yml:
baseurl: "/blog-analytics"
url: "https://username.github.io"

# 3. Subir c√≥digo
git push

# 4. En GitHub: Settings > Pages > Source: main branch
# Sitio disponible en: https://username.github.io/blog-analytics
```

---

## üîÑ Funciones Implementadas

### **1. Procesamiento de Datos con Spark**

```python
def process_clickstream(df):
    """
    Procesa datos de clickstream con ventanas temporales.
    
    Args:
        df: DataFrame de Spark con columnas Timestamp, User_ID, Clicks
    
    Returns:
        DataFrame agregado por ventanas de 1 minuto
    """
    df = df.withColumn("Timestamp", to_timestamp(col("Timestamp")))
    
    windowed = df.groupBy(
        window("Timestamp", "1 minute"), 
        "User_ID"
    ).agg(spark_sum("Clicks").alias("clicks_ventana"))
    
    return windowed
```

### **2. Generaci√≥n de Visualizaciones**

```python
def generate_visualizations(csv_path, output_dir):
    """
    Genera 5 gr√°ficas de an√°lisis de clickstream.
    
    Visualizaciones:
    - Top 15 usuarios por actividad
    - An√°lisis temporal (serie de tiempo)
    - Correlaci√≥n clicks vs sesiones
    - Distribuci√≥n de usuarios
    - Mapa de calor de actividad
    """
    df = pd.read_csv(csv_path, parse_dates=['Timestamp'])
    
    # Gr√°fica 1: Top usuarios
    top_users = df.groupby('User_ID')['Clicks'].sum().sort_values(ascending=False).head(15)
    plt.figure(figsize=(10,6))
    top_users.plot.bar(color='#667eea')
    plt.savefig(f'{output_dir}/top_users_chart.png', dpi=150)
    
    # [... m√°s visualizaciones ...]
```

### **3. Sistema de Layouts Modulares**

- **`default.html`**: Layout base con header/footer
- **`post.html`**: Layout espec√≠fico para art√≠culos con meta tags SEO
- **Componentes reutilizables**: `head.html`, `footer.html`

### **4. Estilos CSS Personalizados**

El archivo `assets/css/style.css` incluye:
- Variables CSS para colores consistentes
- Gradientes modernos (#667eea ‚Üí #764ba2)
- Animaciones (fadeInUp, shimmer)
- Cards hover con efectos 3D
- Responsive design (mobile-first)

---

## ü§î Reflexi√≥n: Streaming vs Procesamiento por Lotes

### **Streaming (Tiempo Real)**

**Ventajas:**
- ‚úÖ Latencia ultra-baja (milisegundos a segundos)
- ‚úÖ Decisiones inmediatas (ofertas en tiempo real)
- ‚úÖ Detecci√≥n instant√°nea de anomal√≠as

**Desventajas:**
- ‚ùå Mayor complejidad de implementaci√≥n
- ‚ùå Costos de infraestructura m√°s altos
- ‚ùå Manejo de estado y ventanas temporales complejo

**Casos de uso ideales:**
- Detecci√≥n de fraude en transacciones
- Alertas de seguridad
- Personalizaci√≥n en tiempo real

---

### **Batch (Por Lotes)**

**Ventajas:**
- ‚úÖ Simplicidad de implementaci√≥n
- ‚úÖ Costos optimizados (procesa off-peak)
- ‚úÖ Ideal para an√°lisis hist√≥ricos complejos

**Desventajas:**
- ‚ùå Latencia alta (minutos a horas)
- ‚ùå No apto para decisiones inmediatas
- ‚ùå Datos "stale" (desactualizados)

**Casos de uso ideales:**
- Reportes diarios/mensuales
- Entrenamiento de modelos ML
- An√°lisis exploratorio de datos

---

### **¬øCu√°l Elegir?**

| Criterio | Streaming | Batch |
|----------|-----------|-------|
| **Latencia requerida** | < 1 segundo | > 1 hora |
| **Volumen de datos** | Flujo continuo | Datasets finitos |
| **Complejidad** | Alta | Media-Baja |
| **Costo** | Alto | Medio |
| **Caso de uso** | Ofertas en tiempo real | Reportes anal√≠ticos |

**Recomendaci√≥n:** En entornos empresariales modernos, lo ideal es una **arquitectura Lambda** que combina ambos enfoques:
- Streaming para decisiones cr√≠ticas en tiempo real
- Batch para an√°lisis profundos y modelos ML

---

## üéì Cierre y Retroalimentaci√≥n

### **Aprendizajes Clave**

1. **Apache Spark es esencial para Big Data**: Procesar 1000+ eventos en tiempo real ser√≠a imposible con pandas puro
2. **Las ventanas temporales son cruciales**: Agregar por minuto/hora permite detectar patrones que ser√≠an invisibles en datos crudos
3. **La visualizaci√≥n cuenta historias**: 5 gr√°ficas bien dise√±adas comunican m√°s que 100 tablas
4. **El streaming es el futuro**: En 2025, las empresas que no procesan datos en tiempo real est√°n en desventaja competitiva

### **Pr√≥ximos Pasos**

- [ ] Implementar modelo predictivo de churn
- [ ] Integrar con Kafka para streaming real
- [ ] Dashboard interactivo con Plotly Dash
- [ ] Sistema de alertas autom√°ticas

---

## üìö Referencias

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Jekyll Documentation](https://jekyllrb.com/docs/)
- [Cayman Theme](https://github.com/pages-themes/cayman)

---

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 12px; color: white; text-align: center; margin-top: 3rem;">
  <h3 style="margin: 0 0 1rem 0;">üí¨ ¬øTienes preguntas o comentarios?</h3>
  <p style="margin: 0; opacity: 0.9;">
    D√©jame un comentario abajo o cont√°ctame. Me encantar√≠a saber c√≥mo aplicaste estos conceptos en tu proyecto.
  </p>
</div>

---

**Autor:** Maria Fernanda Herazo Escobar  
**Curso:** Anal√≠tica Avanzada 2025  
**Fecha:** 29 de Octubre, 2025  
**Tecnolog√≠as:** Apache Spark ‚Ä¢ Python ‚Ä¢ Jekyll ‚Ä¢ GitHub Pages